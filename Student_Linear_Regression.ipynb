{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Student Linear Regression",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KK3tEPlby1_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u3IpMDohlgp"
      },
      "source": [
        "#import Dataset\n",
        "data = pd.read_csv(\"/content/Student DataSet - Sheet1.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "XUCobnVXvo5C",
        "outputId": "7fa6ffb7-94a2-40cd-d4fa-d1ec5ef70206"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ROLL</th>\n",
              "      <th>NAME</th>\n",
              "      <th>Attendance</th>\n",
              "      <th>SUBJECT\\nCREDIT\\nPOINTS</th>\n",
              "      <th>Assignment1</th>\n",
              "      <th>Assignment2</th>\n",
              "      <th>Assignment3</th>\n",
              "      <th>Assignment4</th>\n",
              "      <th>CA1</th>\n",
              "      <th>CA2</th>\n",
              "      <th>CA3</th>\n",
              "      <th>CA4</th>\n",
              "      <th>Final Semester\\nPercentage(%)</th>\n",
              "      <th>GRADE</th>\n",
              "      <th>GRADE\\nPOINT</th>\n",
              "      <th>CREDIT\\nPOINT</th>\n",
              "      <th>GPA</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Leilani Beach</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>27</td>\n",
              "      <td>25</td>\n",
              "      <td>29</td>\n",
              "      <td>40</td>\n",
              "      <td>37</td>\n",
              "      <td>41</td>\n",
              "      <td>47</td>\n",
              "      <td>87.35</td>\n",
              "      <td>E</td>\n",
              "      <td>9</td>\n",
              "      <td>27</td>\n",
              "      <td>8.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Lauryn Dixon</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>29</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>15</td>\n",
              "      <td>34</td>\n",
              "      <td>36</td>\n",
              "      <td>39</td>\n",
              "      <td>21</td>\n",
              "      <td>67.69</td>\n",
              "      <td>B</td>\n",
              "      <td>7</td>\n",
              "      <td>21</td>\n",
              "      <td>6.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Sian Vega</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>25</td>\n",
              "      <td>11</td>\n",
              "      <td>25</td>\n",
              "      <td>18</td>\n",
              "      <td>32</td>\n",
              "      <td>35</td>\n",
              "      <td>59.87</td>\n",
              "      <td>C</td>\n",
              "      <td>6</td>\n",
              "      <td>18</td>\n",
              "      <td>5.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Shoaib Ortega</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>49</td>\n",
              "      <td>20</td>\n",
              "      <td>27</td>\n",
              "      <td>45</td>\n",
              "      <td>79.21</td>\n",
              "      <td>A</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "      <td>7.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Anayah Warren</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>27</td>\n",
              "      <td>21</td>\n",
              "      <td>15</td>\n",
              "      <td>42</td>\n",
              "      <td>47</td>\n",
              "      <td>39</td>\n",
              "      <td>30</td>\n",
              "      <td>77.83</td>\n",
              "      <td>A</td>\n",
              "      <td>8</td>\n",
              "      <td>24</td>\n",
              "      <td>7.83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ROLL           NAME  Attendance  ...  GRADE\\nPOINT  CREDIT\\nPOINT   GPA\n",
              "0     1  Leilani Beach           5  ...             9             27  8.80\n",
              "1     2   Lauryn Dixon           4  ...             7             21  6.82\n",
              "2     3      Sian Vega           4  ...             6             18  5.99\n",
              "3     4  Shoaib Ortega           5  ...             8             24  7.94\n",
              "4     5  Anayah Warren           3  ...             8             24  7.83\n",
              "\n",
              "[5 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ix2ZI4mxlQ5"
      },
      "source": [
        "#selection of value we will predict\n",
        "predict = 'Final Semester\\nPercentage(%)'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "x5TlEw-Ax_fF",
        "outputId": "7ea9f970-5dfe-48e7-e94f-3a151fde5a88"
      },
      "source": [
        "#selection of column we will use\n",
        "model_data = data[['Attendance','Assignment1','Assignment2','Assignment3','Assignment4','CA1','CA2','CA3','CA4','Final Semester\\nPercentage(%)']]\n",
        "model_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Attendance</th>\n",
              "      <th>Assignment1</th>\n",
              "      <th>Assignment2</th>\n",
              "      <th>Assignment3</th>\n",
              "      <th>Assignment4</th>\n",
              "      <th>CA1</th>\n",
              "      <th>CA2</th>\n",
              "      <th>CA3</th>\n",
              "      <th>CA4</th>\n",
              "      <th>Final Semester\\nPercentage(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>27</td>\n",
              "      <td>25</td>\n",
              "      <td>29</td>\n",
              "      <td>40</td>\n",
              "      <td>37</td>\n",
              "      <td>41</td>\n",
              "      <td>47</td>\n",
              "      <td>87.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>29</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>15</td>\n",
              "      <td>34</td>\n",
              "      <td>36</td>\n",
              "      <td>39</td>\n",
              "      <td>21</td>\n",
              "      <td>67.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>15</td>\n",
              "      <td>17</td>\n",
              "      <td>25</td>\n",
              "      <td>11</td>\n",
              "      <td>25</td>\n",
              "      <td>18</td>\n",
              "      <td>32</td>\n",
              "      <td>35</td>\n",
              "      <td>59.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>30</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>29</td>\n",
              "      <td>49</td>\n",
              "      <td>20</td>\n",
              "      <td>27</td>\n",
              "      <td>45</td>\n",
              "      <td>79.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>27</td>\n",
              "      <td>21</td>\n",
              "      <td>15</td>\n",
              "      <td>42</td>\n",
              "      <td>47</td>\n",
              "      <td>39</td>\n",
              "      <td>30</td>\n",
              "      <td>77.83</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Attendance  Assignment1  ...  CA4  Final Semester\\nPercentage(%)\n",
              "0           5           23  ...   47                          87.35\n",
              "1           4           29  ...   21                          67.69\n",
              "2           4           15  ...   35                          59.87\n",
              "3           5           30  ...   45                          79.21\n",
              "4           3           25  ...   30                          77.83\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-CwvtHgl5Yd5",
        "outputId": "622f0dc9-12fd-4132-e759-1004b833e583"
      },
      "source": [
        "model_data = shuffle(model_data)\n",
        "model_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Attendance</th>\n",
              "      <th>Assignment1</th>\n",
              "      <th>Assignment2</th>\n",
              "      <th>Assignment3</th>\n",
              "      <th>Assignment4</th>\n",
              "      <th>CA1</th>\n",
              "      <th>CA2</th>\n",
              "      <th>CA3</th>\n",
              "      <th>CA4</th>\n",
              "      <th>Final Semester\\nPercentage(%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>35</td>\n",
              "      <td>40</td>\n",
              "      <td>22</td>\n",
              "      <td>58.81</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>12</td>\n",
              "      <td>24</td>\n",
              "      <td>20</td>\n",
              "      <td>32</td>\n",
              "      <td>29</td>\n",
              "      <td>47</td>\n",
              "      <td>39</td>\n",
              "      <td>73.64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>2</td>\n",
              "      <td>28</td>\n",
              "      <td>9</td>\n",
              "      <td>28</td>\n",
              "      <td>16</td>\n",
              "      <td>29</td>\n",
              "      <td>16</td>\n",
              "      <td>34</td>\n",
              "      <td>41</td>\n",
              "      <td>61.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>26</td>\n",
              "      <td>25</td>\n",
              "      <td>30</td>\n",
              "      <td>36</td>\n",
              "      <td>36</td>\n",
              "      <td>39</td>\n",
              "      <td>34</td>\n",
              "      <td>73.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>29</td>\n",
              "      <td>27</td>\n",
              "      <td>24</td>\n",
              "      <td>49</td>\n",
              "      <td>40</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>65.41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Attendance  Assignment1  ...  CA4  Final Semester\\nPercentage(%)\n",
              "37            1           14  ...   22                          58.81\n",
              "492           2           23  ...   39                          73.64\n",
              "287           2           28  ...   41                          61.40\n",
              "528           3           15  ...   34                          73.86\n",
              "235           2            5  ...   16                          65.41\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwH5duoBS6FL",
        "outputId": "18287fd3-b615-430f-9b29-c143cbb1c0d1"
      },
      "source": [
        "labels = model_data[predict]\n",
        "\n",
        "# Labelling the strings of predict column\n",
        "\n",
        "le = sklearn.preprocessing.LabelEncoder()\n",
        "\n",
        "model_data[predict] = le.fit_transform(labels)\n",
        "arr = labels.unique()\n",
        "\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      E\n",
              "1      B\n",
              "2      C\n",
              "3      A\n",
              "4      A\n",
              "      ..\n",
              "851    D\n",
              "852    A\n",
              "853    B\n",
              "854    C\n",
              "855    C\n",
              "Name: GRADE, Length: 856, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_POXoLdxYrl"
      },
      "source": [
        "# separating Train and Test data\n",
        "\n",
        "x = np.array(model_data.drop([predict],axis='columns'))\n",
        "y = np.array(model_data[predict])\n",
        "\n",
        "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.2)\n",
        "\n",
        "############################################\n",
        "\n",
        "##x = model_data.drop([predict],axis='columns')\n",
        "##y = model_data[predict] \n",
        "\n",
        "##x_train_List = x.head(648)\n",
        "##x_test_List = x.tail(172)\n",
        "\n",
        "##y_train_List = y.head(648)\n",
        "##y_test_List = y.tail(172)\n",
        "\n",
        "##x_train = np.array(x_train_List)\n",
        "##y_train = np.array(y_train_List)\n",
        "##x_test = np.array(x_test_List)\n",
        "##y_test = np.array(y_test_List)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDH_ZTkzWJrC"
      },
      "source": [
        "x = np.array(model_data.drop([predict],axis='columns'))\n",
        "y = np.array(model_data[predict])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiEA2sH9SfWw",
        "outputId": "09297c23-1074-44ab-e460-5eb78bcf14c1"
      },
      "source": [
        "#Training Model multiple times for higher accuracy\n",
        "best = 0\n",
        "for _ in range(200):\n",
        "    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.2)\n",
        "\n",
        "    linear = linear_model.LinearRegression()\n",
        "\n",
        "    linear.fit(x_train, y_train)\n",
        "    acc = linear.score(x_test, y_test)\n",
        "    print(\"Accuracy: \" + str(acc))\n",
        "\n",
        "    # Save the highest accuracy\n",
        "    if (acc > best):\n",
        "        best = acc\n",
        "        with open(\"studentgrades.pickle\", \"wb\") as f:\n",
        "            pickle.dump(linear, f)\n",
        "print(\"Highest Accuracy:\", best)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9273722385408354\n",
            "Accuracy: 0.9006530744187492\n",
            "Accuracy: 0.9087916675043519\n",
            "Accuracy: 0.9031764891365913\n",
            "Accuracy: 0.9006475194615973\n",
            "Accuracy: 0.8974215190879983\n",
            "Accuracy: 0.8934401085169701\n",
            "Accuracy: 0.9089101122143252\n",
            "Accuracy: 0.9103595211603853\n",
            "Accuracy: 0.9105516360722885\n",
            "Accuracy: 0.9159788083703976\n",
            "Accuracy: 0.9040386725404885\n",
            "Accuracy: 0.9115788143103767\n",
            "Accuracy: 0.9048552565083781\n",
            "Accuracy: 0.899477840033494\n",
            "Accuracy: 0.9088351217700114\n",
            "Accuracy: 0.919884165021099\n",
            "Accuracy: 0.9124570768324834\n",
            "Accuracy: 0.8959124953766492\n",
            "Accuracy: 0.9032663289470366\n",
            "Accuracy: 0.9057929665187867\n",
            "Accuracy: 0.9192583330464545\n",
            "Accuracy: 0.9074234024721338\n",
            "Accuracy: 0.9164073769118078\n",
            "Accuracy: 0.9032603413210982\n",
            "Accuracy: 0.9186692017678241\n",
            "Accuracy: 0.9137357654696932\n",
            "Accuracy: 0.9173335071028581\n",
            "Accuracy: 0.8778585585191963\n",
            "Accuracy: 0.9137544610234646\n",
            "Accuracy: 0.9403650210061103\n",
            "Accuracy: 0.9208847823313479\n",
            "Accuracy: 0.90407257428731\n",
            "Accuracy: 0.9191248356686226\n",
            "Accuracy: 0.9062815155251522\n",
            "Accuracy: 0.8911768505379835\n",
            "Accuracy: 0.8825882847041655\n",
            "Accuracy: 0.9078011823133162\n",
            "Accuracy: 0.9173158940557296\n",
            "Accuracy: 0.9142916928223793\n",
            "Accuracy: 0.9166764586883137\n",
            "Accuracy: 0.9313667150219337\n",
            "Accuracy: 0.91889284085206\n",
            "Accuracy: 0.8942395541746861\n",
            "Accuracy: 0.9075870750671758\n",
            "Accuracy: 0.9195436079446623\n",
            "Accuracy: 0.9046496147921729\n",
            "Accuracy: 0.9122447282246325\n",
            "Accuracy: 0.9139999807136645\n",
            "Accuracy: 0.9016308489975563\n",
            "Accuracy: 0.9267066294999887\n",
            "Accuracy: 0.9094222903284216\n",
            "Accuracy: 0.9116509527340199\n",
            "Accuracy: 0.915641441889432\n",
            "Accuracy: 0.9024764950651745\n",
            "Accuracy: 0.9210374934463604\n",
            "Accuracy: 0.9080512725433708\n",
            "Accuracy: 0.9263340179822667\n",
            "Accuracy: 0.9131684885731142\n",
            "Accuracy: 0.9284586571385259\n",
            "Accuracy: 0.9050578563005759\n",
            "Accuracy: 0.9247769250745005\n",
            "Accuracy: 0.9098477691214104\n",
            "Accuracy: 0.9195960657289046\n",
            "Accuracy: 0.9221652461082095\n",
            "Accuracy: 0.9157474624539018\n",
            "Accuracy: 0.8994652272158657\n",
            "Accuracy: 0.9065337720099611\n",
            "Accuracy: 0.9025986250159264\n",
            "Accuracy: 0.926082663098164\n",
            "Accuracy: 0.9123530113190597\n",
            "Accuracy: 0.9123238329841188\n",
            "Accuracy: 0.8863295267768387\n",
            "Accuracy: 0.9228796079019134\n",
            "Accuracy: 0.8836576831652783\n",
            "Accuracy: 0.9037900820596755\n",
            "Accuracy: 0.9022091026768111\n",
            "Accuracy: 0.9044760139584394\n",
            "Accuracy: 0.9084547037594963\n",
            "Accuracy: 0.90468621893438\n",
            "Accuracy: 0.9275648622054512\n",
            "Accuracy: 0.9119186346337007\n",
            "Accuracy: 0.9175926147215797\n",
            "Accuracy: 0.9178882114927897\n",
            "Accuracy: 0.9260297648500677\n",
            "Accuracy: 0.9228728835463552\n",
            "Accuracy: 0.9047897685439863\n",
            "Accuracy: 0.9244535284250655\n",
            "Accuracy: 0.9264539309323544\n",
            "Accuracy: 0.9039920548560518\n",
            "Accuracy: 0.901083941473428\n",
            "Accuracy: 0.8967677533575233\n",
            "Accuracy: 0.9274080273272847\n",
            "Accuracy: 0.898547302555403\n",
            "Accuracy: 0.9088862410580831\n",
            "Accuracy: 0.9019998627492123\n",
            "Accuracy: 0.890267811085469\n",
            "Accuracy: 0.9208484480941443\n",
            "Accuracy: 0.9147965028913356\n",
            "Accuracy: 0.9074597301147367\n",
            "Accuracy: 0.8972635466744104\n",
            "Accuracy: 0.9267309292292335\n",
            "Accuracy: 0.8975661180394472\n",
            "Accuracy: 0.8880118825806763\n",
            "Accuracy: 0.9180620048197424\n",
            "Accuracy: 0.9113500427474167\n",
            "Accuracy: 0.9122337214460798\n",
            "Accuracy: 0.9077682173070291\n",
            "Accuracy: 0.9060735021296986\n",
            "Accuracy: 0.8943367981076998\n",
            "Accuracy: 0.9144264709683688\n",
            "Accuracy: 0.9058892378977094\n",
            "Accuracy: 0.9224378143186539\n",
            "Accuracy: 0.9109818160594825\n",
            "Accuracy: 0.9088780681722468\n",
            "Accuracy: 0.9059455926484361\n",
            "Accuracy: 0.921718477338954\n",
            "Accuracy: 0.8910409373969171\n",
            "Accuracy: 0.9092147829249144\n",
            "Accuracy: 0.9090305909479934\n",
            "Accuracy: 0.9294864192899005\n",
            "Accuracy: 0.9234780420966582\n",
            "Accuracy: 0.9067197611787082\n",
            "Accuracy: 0.9094840912570992\n",
            "Accuracy: 0.8993176032262218\n",
            "Accuracy: 0.9079335194362778\n",
            "Accuracy: 0.8871176106941971\n",
            "Accuracy: 0.885715965913917\n",
            "Accuracy: 0.9272674116202155\n",
            "Accuracy: 0.9098092958986493\n",
            "Accuracy: 0.8953603245516231\n",
            "Accuracy: 0.9000794939338548\n",
            "Accuracy: 0.9146795497581766\n",
            "Accuracy: 0.9271217125766851\n",
            "Accuracy: 0.9206017575900453\n",
            "Accuracy: 0.9196690961445878\n",
            "Accuracy: 0.9028035382290316\n",
            "Accuracy: 0.8987131815855017\n",
            "Accuracy: 0.9140733803568857\n",
            "Accuracy: 0.9059870207585538\n",
            "Accuracy: 0.9069996918798321\n",
            "Accuracy: 0.9244201545046897\n",
            "Accuracy: 0.8724693141710789\n",
            "Accuracy: 0.9146811081113839\n",
            "Accuracy: 0.9080733283536513\n",
            "Accuracy: 0.9092131642064334\n",
            "Accuracy: 0.911284160338028\n",
            "Accuracy: 0.9202183622639772\n",
            "Accuracy: 0.8931706847256256\n",
            "Accuracy: 0.9266311836869324\n",
            "Accuracy: 0.8964381550609234\n",
            "Accuracy: 0.9019828723917958\n",
            "Accuracy: 0.9103110779370005\n",
            "Accuracy: 0.9210082905866924\n",
            "Accuracy: 0.90208685039641\n",
            "Accuracy: 0.9145032628783746\n",
            "Accuracy: 0.901979031307104\n",
            "Accuracy: 0.9027154868465971\n",
            "Accuracy: 0.8988695771533242\n",
            "Accuracy: 0.9118924038579801\n",
            "Accuracy: 0.909355630344885\n",
            "Accuracy: 0.9187086130414184\n",
            "Accuracy: 0.9233611128825937\n",
            "Accuracy: 0.9120215960820973\n",
            "Accuracy: 0.8992172461960171\n",
            "Accuracy: 0.9191370890499976\n",
            "Accuracy: 0.9260994164281331\n",
            "Accuracy: 0.8940323302462999\n",
            "Accuracy: 0.9084096953303858\n",
            "Accuracy: 0.9175392207956046\n",
            "Accuracy: 0.8905736067027995\n",
            "Accuracy: 0.8856242913800225\n",
            "Accuracy: 0.9231685818604101\n",
            "Accuracy: 0.9179277710978827\n",
            "Accuracy: 0.8937843673380056\n",
            "Accuracy: 0.9154501999163845\n",
            "Accuracy: 0.9167837185361265\n",
            "Accuracy: 0.9008588809621282\n",
            "Accuracy: 0.8991956231801738\n",
            "Accuracy: 0.9157573933282923\n",
            "Accuracy: 0.8994973924701167\n",
            "Accuracy: 0.9140959543082721\n",
            "Accuracy: 0.8818606181999125\n",
            "Accuracy: 0.8749061164426954\n",
            "Accuracy: 0.9149603573234835\n",
            "Accuracy: 0.9039703349078193\n",
            "Accuracy: 0.9138458823894018\n",
            "Accuracy: 0.9249158644073165\n",
            "Accuracy: 0.9142930763026409\n",
            "Accuracy: 0.916884323367631\n",
            "Accuracy: 0.8959320731946396\n",
            "Accuracy: 0.9146360324896293\n",
            "Accuracy: 0.9154781143217325\n",
            "Accuracy: 0.9225550982350276\n",
            "Accuracy: 0.9169074583477347\n",
            "Accuracy: 0.885671556008246\n",
            "Accuracy: 0.9061625605548146\n",
            "Accuracy: 0.8991664139877629\n",
            "Accuracy: 0.9312349144359107\n",
            "Accuracy: 0.8977593951604963\n",
            "Highest Accuracy: 0.9403650210061103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlOrQedPgmvY",
        "outputId": "ed26c8e7-0a3d-4936-e827-6afdfe1013cd"
      },
      "source": [
        "# Load model\n",
        "pickle_in = open(\"studentgrades.pickle\", \"rb\")\n",
        "linear = pickle.load(pickle_in)\n",
        "\n",
        "print(\"-------------------------\")\n",
        "print('Coefficient: \\n', linear.coef_)\n",
        "print('Intercept: \\n', linear.intercept_)\n",
        "print(\"-------------------------\")\n",
        "\n",
        "predictions = linear.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------\n",
            "Coefficient: \n",
            " [2.34164526 0.07096663 0.08110808 0.10129944 0.06728655 0.36036134\n",
            " 0.31367497 0.34628465 0.339184  ]\n",
            "Intercept: \n",
            " 10.202855781727607\n",
            "-------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht7jVxwXpmMR",
        "outputId": "35bc4ca4-a3f7-44cf-9a39-db7c89964cbd"
      },
      "source": [
        "# Print the predictions, the variables we used and the actual final Percentage\n",
        "for x in range(len(predictions)):\n",
        "    print(\"Predicted Grade:\", predictions[x], \"Data:\", x_test[x], \"Grade:\", y_test[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Grade: 63.84435848649649 Data: [ 1 13  9 29 26 16 19 47 50] Grade: 67.99\n",
            "Predicted Grade: 54.5550529427247 Data: [ 1 23 25 26 17 24 40 21 18] Grade: 57.28\n",
            "Predicted Grade: 63.46566402362643 Data: [ 5 11 26 26 21 45 26 10 20] Grade: 65.68\n",
            "Predicted Grade: 67.0550266625907 Data: [ 3 21 15 23 27 38 19 36 32] Grade: 62.52\n",
            "Predicted Grade: 58.05284481647725 Data: [ 4 15 12 12 11 36 15 27 22] Grade: 55.5\n",
            "Predicted Grade: 74.25484811568425 Data: [ 3 15 26 25 30 36 36 39 34] Grade: 73.86\n",
            "Predicted Grade: 70.94245020807062 Data: [ 1 11 23 21 25 37 40 40 36] Grade: 70.64\n",
            "Predicted Grade: 63.8498044584808 Data: [ 4 11 17 22 25 22 29 22 40] Grade: 66.73\n",
            "Predicted Grade: 62.01570752029825 Data: [ 3 15 17 12 21  9 29 34 46] Grade: 60.15\n",
            "Predicted Grade: 59.0577089926612 Data: [ 2 13 19 28 19 21 36 11 44] Grade: 59.38\n",
            "Predicted Grade: 64.4896315578022 Data: [ 5 15 28 30 17 29 27 28 19] Grade: 60.13\n",
            "Predicted Grade: 57.387884007492296 Data: [ 2 23 27 27 27 20 40 18 24] Grade: 60.05\n",
            "Predicted Grade: 46.403165630936385 Data: [ 1  9 19 11 19 13 37 15 23] Grade: 49.42\n",
            "Predicted Grade: 59.832012672296166 Data: [ 3 12  9 29 25 15 27 20 46] Grade: 65.59\n",
            "Predicted Grade: 60.37412492092822 Data: [ 2 20 26 21 22 34 32 17 30] Grade: 54.9\n",
            "Predicted Grade: 75.53640035364809 Data: [ 5 26 21 14  1 38 37 32 36] Grade: 74.99\n",
            "Predicted Grade: 64.17470872045024 Data: [ 4 21 27 20 16 27 37 29 19] Grade: 62.63\n",
            "Predicted Grade: 62.36966815559471 Data: [ 2  5 26 24  9 50 16 41 14] Grade: 67.26\n",
            "Predicted Grade: 77.84142630026662 Data: [ 4 20 26 21 23 41 36 34 39] Grade: 76.98\n",
            "Predicted Grade: 55.5133066340176 Data: [ 3 17 25 14  6 19 37 27 16] Grade: 57.15\n",
            "Predicted Grade: 74.34048145135391 Data: [ 4 14 17 28 30 27 31 39 43] Grade: 75.6\n",
            "Predicted Grade: 65.04670078141464 Data: [ 2 22 11 12 15 34 32 18 50] Grade: 66.58\n",
            "Predicted Grade: 65.17601803705199 Data: [ 4 18 19 17 29 19 34 33 30] Grade: 60.87\n",
            "Predicted Grade: 57.92851261352983 Data: [ 1 15  6 19 10 39 24 45 12] Grade: 56.81\n",
            "Predicted Grade: 78.8824365471027 Data: [ 5 23 16 23  9 42 39 47 22] Grade: 76.29\n",
            "Predicted Grade: 82.51446033072587 Data: [ 4 13 15 16 25 50 45 38 36] Grade: 84.9\n",
            "Predicted Grade: 82.98527423599582 Data: [ 5 27 10 30  5 42 34 48 37] Grade: 86.55\n",
            "Predicted Grade: 64.5998817133245 Data: [ 2 17 25 13 30 19 45 19 46] Grade: 68.73\n",
            "Predicted Grade: 54.718042825225204 Data: [ 2 28 19 30 25 24 37 19 14] Grade: 55.82\n",
            "Predicted Grade: 51.38320352470641 Data: [ 2 20 30 29 16 22 17 15 30] Grade: 50.82\n",
            "Predicted Grade: 75.2716280711521 Data: [ 4 11 17 13  9 50 44 18 40] Grade: 71.46\n",
            "Predicted Grade: 77.88698314464196 Data: [ 3 25 27 21 15 42 47 39 30] Grade: 77.83\n",
            "Predicted Grade: 74.89439217161059 Data: [ 2 29 16 11 14 50 39 37 34] Grade: 78.85\n",
            "Predicted Grade: 59.90734223664192 Data: [ 5  9 18 13 17 23 12 50 12] Grade: 67.6\n",
            "Predicted Grade: 69.35468099486064 Data: [ 3 27 26 26 20 46 26  8 49] Grade: 66.17\n",
            "Predicted Grade: 62.65708723845655 Data: [ 4 24 16 23 16 14 43 24 29] Grade: 64.0\n",
            "Predicted Grade: 84.35808239960497 Data: [ 3 17 24 18 30 49 36 45 46] Grade: 84.16\n",
            "Predicted Grade: 63.07972751195963 Data: [ 5 13 25 30 25 17 16 43 22] Grade: 67.83\n",
            "Predicted Grade: 75.13319864663177 Data: [ 2 27 13 25 15 39 47 25 48] Grade: 73.92\n",
            "Predicted Grade: 56.5457500492222 Data: [ 1 21 29 22 29 22 22 17 45] Grade: 61.44\n",
            "Predicted Grade: 62.90321120526898 Data: [ 3 15 15 29 29 25 36 32 21] Grade: 62.96\n",
            "Predicted Grade: 78.98425767891611 Data: [ 4 13 23 27 26 26 38 46 44] Grade: 77.24\n",
            "Predicted Grade: 70.8310717860057 Data: [ 2 21 24 15 24 45 26 34 39] Grade: 70.33\n",
            "Predicted Grade: 72.01322446167688 Data: [ 3  7  6 20 11 47 42 33 28] Grade: 73.06\n",
            "Predicted Grade: 62.06624923554239 Data: [ 5 29 26 25 25 31 45  6 13] Grade: 64.38\n",
            "Predicted Grade: 61.76859018822814 Data: [ 1 22  0 18 18 11 50 31 42] Grade: 61.56\n",
            "Predicted Grade: 62.44069339043839 Data: [ 3 13 13 29 19 30 33 31 21] Grade: 58.78\n",
            "Predicted Grade: 62.90265695446206 Data: [ 4 22 11 22 20 17 34 23 37] Grade: 61.44\n",
            "Predicted Grade: 55.05430796059539 Data: [ 3  9 21 15 28 41 18 18 16] Grade: 60.11\n",
            "Predicted Grade: 74.2405331084762 Data: [ 4 18 18 27 21 37 42 35 27] Grade: 72.85\n",
            "Predicted Grade: 81.26065958269257 Data: [ 5 18 29 20 29 33 42 31 47] Grade: 84.02\n",
            "Predicted Grade: 62.94145437728725 Data: [ 4 21 15 22 19 29 37 25 19] Grade: 61.79\n",
            "Predicted Grade: 68.60676727287978 Data: [ 3 30 16 24 21 24 28 31 47] Grade: 72.7\n",
            "Predicted Grade: 59.261733972415826 Data: [ 4  6 21 29  7 24  6 31 38] Grade: 55.77\n",
            "Predicted Grade: 59.93127946307673 Data: [ 1 27 26 16 17 34 26 25 34] Grade: 58.85\n",
            "Predicted Grade: 82.41516517094051 Data: [ 4 30 21 23 29 49 32 34 45] Grade: 84.57\n",
            "Predicted Grade: 53.73872210904744 Data: [ 3 26 24 28  7 14 21 20 32] Grade: 53.04\n",
            "Predicted Grade: 52.41961122902792 Data: [ 1  8  8 11 18 27 17 34 28] Grade: 48.86\n",
            "Predicted Grade: 82.6238687210657 Data: [ 3 24 26 30 21 40 41 49 38] Grade: 85.02\n",
            "Predicted Grade: 73.2121651109877 Data: [ 5 18 20 12 21 39 40 25 31] Grade: 71.41\n",
            "Predicted Grade: 71.67720179553274 Data: [ 4 15 12 20 17 46 31 35 25] Grade: 72.59\n",
            "Predicted Grade: 79.36584159003488 Data: [ 4 29 17 19 17 40 19 50 46] Grade: 75.4\n",
            "Predicted Grade: 43.5299729702443 Data: [ 1 18 27 24 15 19 11 30 10] Grade: 43.58\n",
            "Predicted Grade: 63.944932962576615 Data: [ 4 10 19 27 23 13 26 34 39] Grade: 61.47\n",
            "Predicted Grade: 72.14868290352277 Data: [ 4 20 17 12 10 46 45 34 16] Grade: 68.36\n",
            "Predicted Grade: 59.62184954444569 Data: [ 4 22 13 17 17 14 29 11 49] Grade: 65.47\n",
            "Predicted Grade: 66.81016187430295 Data: [ 2 30 23 24 22 33 39 31 27] Grade: 66.35\n",
            "Predicted Grade: 64.11484050410706 Data: [ 5 20 28  7 11 26 18 48 16] Grade: 71.19\n",
            "Predicted Grade: 53.60799465777405 Data: [ 1 29 26 23 16 13 47 22 19] Grade: 59.97\n",
            "Predicted Grade: 79.20271598573908 Data: [ 4 26 26 30 26 37 43 46 24] Grade: 76.32\n",
            "Predicted Grade: 71.97390155635446 Data: [ 1 18 13 22 25 50 38 26 42] Grade: 72.27\n",
            "Predicted Grade: 70.91021758797814 Data: [ 3 12 24 26 24 30 48 11 50] Grade: 68.67\n",
            "Predicted Grade: 50.0303909183885 Data: [ 2 12 26 10 15 46 14  5 22] Grade: 54.03\n",
            "Predicted Grade: 71.73795579140608 Data: [ 5 21 23 12 21 30 42 26 32] Grade: 73.07\n",
            "Predicted Grade: 76.02720462608293 Data: [ 4 24 29 26 17 30 31 39 43] Grade: 77.01\n",
            "Predicted Grade: 64.99807300490374 Data: [ 5  3 15 25 14 10 24 36 43] Grade: 63.61\n",
            "Predicted Grade: 76.54558661207851 Data: [ 2 14 23 21 30 37 42 47 35] Grade: 78.91\n",
            "Predicted Grade: 59.761986417711405 Data: [ 4  8 22 14 19 12 11 33 47] Grade: 64.83\n",
            "Predicted Grade: 52.49065043522661 Data: [ 2 27 15  9 21 16 39 34  7] Grade: 52.77\n",
            "Predicted Grade: 60.79593159401194 Data: [ 4 22 29 29 14 18 24 12 45] Grade: 65.38\n",
            "Predicted Grade: 56.362901336396796 Data: [ 1 16 27 14 15 44 23 10 34] Grade: 55.5\n",
            "Predicted Grade: 69.94206401804108 Data: [ 3 21 23 27 19 19 33 47 35] Grade: 69.45\n",
            "Predicted Grade: 65.28650410025935 Data: [ 3 18 29 14 29 16 45 21 41] Grade: 68.2\n",
            "Predicted Grade: 67.43708475069397 Data: [ 3 28 21 12 20 26 35 29 40] Grade: 68.23\n",
            "Predicted Grade: 58.704423266447925 Data: [ 3 11 26 21 19 46 20 17 19] Grade: 64.64\n",
            "Predicted Grade: 77.87853995505111 Data: [ 5 27 22 19 29 39 31 27 45] Grade: 79.12\n",
            "Predicted Grade: 60.0512402539756 Data: [ 3 45  6 17 27 37 35 14 19] Grade: 62.07\n",
            "Predicted Grade: 72.59069632669593 Data: [ 2 28 26 19 30 45 50 18 34] Grade: 71.73\n",
            "Predicted Grade: 68.31036431969441 Data: [ 2 22 26 20 28 13 45 35 44] Grade: 63.8\n",
            "Predicted Grade: 58.325244860181485 Data: [ 2 13 24 18 17 25 34 41 11] Grade: 55.89\n",
            "Predicted Grade: 60.96128297331296 Data: [ 2 11 11 18 28 24 28 28 40] Grade: 63.14\n",
            "Predicted Grade: 58.52813552127961 Data: [ 2 30  1  3  7 15 50 33 24] Grade: 64.53\n",
            "Predicted Grade: 70.4402387993197 Data: [ 3 12 21 17 15 34 34 35 38] Grade: 69.92\n",
            "Predicted Grade: 58.56539109327748 Data: [ 2 25 27 11 15 25 43 24 20] Grade: 63.5\n",
            "Predicted Grade: 54.14590343004784 Data: [ 1 25  8 15 27 18 31 42 15] Grade: 58.16\n",
            "Predicted Grade: 55.61378889843431 Data: [ 3 12 25 25 24 12 25 28 28] Grade: 49.01\n",
            "Predicted Grade: 63.125390097996 Data: [ 1  9 16 15 23 38 46 22 29] Grade: 64.65\n",
            "Predicted Grade: 67.64026429992069 Data: [ 5 14 23 17 15 26 39 31 23] Grade: 67.98\n",
            "Predicted Grade: 67.0913760338974 Data: [ 4 20 30 12 22 43 19 26 31] Grade: 68.31\n",
            "Predicted Grade: 63.4192796096128 Data: [ 1 18 19 20 19 42 31 31 27] Grade: 63.63\n",
            "Predicted Grade: 68.56304103987574 Data: [ 4 24 20 30 26 27 49 25 21] Grade: 74.37\n",
            "Predicted Grade: 72.35576037069552 Data: [ 3 19 11 26 28 38 23 46 34] Grade: 72.33\n",
            "Predicted Grade: 55.29730922795913 Data: [ 1 25 30 15 30 20 31 14 39] Grade: 56.5\n",
            "Predicted Grade: 54.958102443232995 Data: [ 4 12 26 15 15 34 29 10 15] Grade: 53.37\n",
            "Predicted Grade: 80.44352953078595 Data: [ 4 17 18 30 25 48 39 30 40] Grade: 81.09\n",
            "Predicted Grade: 78.16733886758792 Data: [ 2 23 21 24 29 44 36 37 46] Grade: 79.06\n",
            "Predicted Grade: 53.27809630537073 Data: [ 1 26 12 27 29 23 29 34 12] Grade: 50.33\n",
            "Predicted Grade: 68.70204736441623 Data: [ 3 30 17 18  9 36 34 27 37] Grade: 66.5\n",
            "Predicted Grade: 59.82032838911087 Data: [ 3 11  0 20  2 28 29 16 44] Grade: 61.12\n",
            "Predicted Grade: 64.22775222670401 Data: [ 1 22 26 18 11 38 43 41 12] Grade: 57.87\n",
            "Predicted Grade: 67.0557592783858 Data: [ 4 23 12 18 26 38 30 32 21] Grade: 65.3\n",
            "Predicted Grade: 80.16171380773231 Data: [ 5 21 19 26 13 39 38 42 33] Grade: 79.07\n",
            "Predicted Grade: 59.905246635009185 Data: [ 3  8 24 14 17 29 44 17 22] Grade: 63.42\n",
            "Predicted Grade: 66.31292794079692 Data: [ 5 28 18 15 23 45 12 38 14] Grade: 68.59\n",
            "Predicted Grade: 72.37899449058496 Data: [ 4 14 11 25 23 35 49 29 26] Grade: 77.42\n",
            "Predicted Grade: 74.94350043405059 Data: [ 3 14 19 14 17 40 42 38 35] Grade: 74.12\n",
            "Predicted Grade: 60.21782510852184 Data: [ 2 26 24 15 21 39 40 19 16] Grade: 60.37\n",
            "Predicted Grade: 73.3442868439918 Data: [ 4 15 30 22 26 19 46 37 36] Grade: 72.24\n",
            "Predicted Grade: 73.29709797711516 Data: [ 2 16 16 25 17 25 42 38 50] Grade: 73.07\n",
            "Predicted Grade: 51.634894171531265 Data: [ 3 30  0 16  3 39 36  3 12] Grade: 52.46\n",
            "Predicted Grade: 59.86421866617368 Data: [ 2 30 24 15 24 18 35 41 18] Grade: 62.49\n",
            "Predicted Grade: 75.18410725446667 Data: [ 4 22 18 29 21 34 36 39 33] Grade: 74.66\n",
            "Predicted Grade: 62.04476887889248 Data: [ 4  7 19 19 29 33 36 21 18] Grade: 61.8\n",
            "Predicted Grade: 65.96474730713612 Data: [ 3 24  8 17  0 41 40  6 45] Grade: 59.01\n",
            "Predicted Grade: 70.85472534331686 Data: [ 3 18 15 20 24 49 46 20 25] Grade: 72.31\n",
            "Predicted Grade: 59.1658189797998 Data: [ 3 14 19 21  8  7 46 16 42] Grade: 60.41\n",
            "Predicted Grade: 59.928301795476386 Data: [ 2 29 15 29 18 45 30 17 18] Grade: 64.92\n",
            "Predicted Grade: 76.09458296040219 Data: [ 2 29 26  3 24 24 40 49 50] Grade: 75.65\n",
            "Predicted Grade: 64.17172970245974 Data: [ 5 23 30 26 26 23 28 17 32] Grade: 60.88\n",
            "Predicted Grade: 61.43314254874161 Data: [ 4  9 15 24 19 19 25 34 29] Grade: 59.43\n",
            "Predicted Grade: 55.028862221672405 Data: [ 1 30 27 15 16 19 27 34 25] Grade: 54.65\n",
            "Predicted Grade: 75.62024644844986 Data: [ 5 10 17 25 24 48 34 25 32] Grade: 78.28\n",
            "Predicted Grade: 51.25090706601084 Data: [ 1 11 11 13 16  8 41 34 21] Grade: 49.54\n",
            "Predicted Grade: 80.32879081235134 Data: [ 5 25 13 17 24 36 50 27 42] Grade: 82.33\n",
            "Predicted Grade: 71.68647579938563 Data: [ 3 12  7 12 25 44 25 46 31] Grade: 71.67\n",
            "Predicted Grade: 49.0490375915715 Data: [ 1 15 25 21 15 28 14 27 19] Grade: 44.92\n",
            "Predicted Grade: 83.35170148643034 Data: [ 1 10 14 23 20 48 15 77 49] Grade: 86.88\n",
            "Predicted Grade: 64.57601802423746 Data: [ 5  6 21 16 15 37 16 31 26] Grade: 61.46\n",
            "Predicted Grade: 65.34999973537089 Data: [ 4  3 17 14 10 30 24 50 19] Grade: 70.17\n",
            "Predicted Grade: 66.05871061419651 Data: [ 2 26 25 24 14 45 16 43 23] Grade: 65.06\n",
            "Predicted Grade: 68.06236668120121 Data: [ 5 26 23 20 18 24 33 27 32] Grade: 65.55\n",
            "Predicted Grade: 80.48151689248161 Data: [ 5 22 15 30 16 49 36 48 18] Grade: 77.22\n",
            "Predicted Grade: 66.20870917101581 Data: [ 3 11  7  6 13 17 38 41 41] Grade: 60.03\n",
            "Predicted Grade: 56.042621444924244 Data: [ 3 28 23 23 24 21 19 27 24] Grade: 52.41\n",
            "Predicted Grade: 84.46682696056946 Data: [ 4 26 27 22 21 46 40 40 42] Grade: 85.85\n",
            "Predicted Grade: 73.88242259195255 Data: [ 5 24 19 21 14 39 34 35 26] Grade: 71.33\n",
            "Predicted Grade: 70.87814621358532 Data: [ 2 13 19 18 18 33 40 39 37] Grade: 69.02\n",
            "Predicted Grade: 76.20202833794586 Data: [ 2 24 29 26 22 39 23 47 46] Grade: 72.21\n",
            "Predicted Grade: 70.75529303505371 Data: [ 4 16 21 20 23 26 31 34 41] Grade: 70.31\n",
            "Predicted Grade: 44.02493858643337 Data: [ 1  6 13 16 25 12 23 33 11] Grade: 45.51\n",
            "Predicted Grade: 60.783529859652766 Data: [ 2 10 19 30 22 32 31 33 19] Grade: 56.05\n",
            "Predicted Grade: 54.039423002476106 Data: [ 4  6 10 27  8 20 43 15 12] Grade: 59.71\n",
            "Predicted Grade: 52.23648186620356 Data: [ 2 28 12 18 21 24 25 11 32] Grade: 48.63\n",
            "Predicted Grade: 69.16147569289242 Data: [ 5  8  9 23 30 11 14 49 48] Grade: 72.93\n",
            "Predicted Grade: 71.49252025572801 Data: [ 2 23 27 23 21 16 46 50 34] Grade: 69.28\n",
            "Predicted Grade: 63.75930972165253 Data: [ 2 26 10 17 25 41 12 27 44] Grade: 61.43\n",
            "Predicted Grade: 45.10444085252908 Data: [ 5  9 20 12 20 19 15  5 15] Grade: 38.4\n",
            "Predicted Grade: 67.97988204886158 Data: [ 5 17 16 16 17 45 15 29 29] Grade: 66.31\n",
            "Predicted Grade: 92.19837838624416 Data: [ 5 25 24 24 27 45 41 48 49] Grade: 91.85\n",
            "Predicted Grade: 86.0016957987558 Data: [ 3 24 24 30 25 44 46 38 50] Grade: 86.38\n",
            "Predicted Grade: 64.44390272420105 Data: [ 2  6  8 24 28 17 11 47 54] Grade: 69.73\n",
            "Predicted Grade: 64.93988890747468 Data: [ 4 14 30 10 18 48 10 41 15] Grade: 67.35\n",
            "Predicted Grade: 70.76402014922976 Data: [ 1 23 22 26 19 44 20 38 46] Grade: 65.94\n",
            "Predicted Grade: 38.596423367061604 Data: [ 1 26  6 17 15 14 16 10 22] Grade: 35.5\n",
            "Predicted Grade: 72.78961197876095 Data: [ 4 19 18 25 16 36 16 45 39] Grade: 67.84\n",
            "Predicted Grade: 76.83677766373461 Data: [ 3 10 21 19 11 48 41 41 30] Grade: 76.47\n",
            "Predicted Grade: 72.02458900439747 Data: [ 4  7 28 17 26 36 34 27 39] Grade: 71.34\n",
            "Predicted Grade: 66.93893202149704 Data: [ 2 21 20 14 17 43 34 29 30] Grade: 68.75\n",
            "Predicted Grade: 63.638803781821814 Data: [ 5 27 11 16 19 11 29 33 34] Grade: 57.72\n",
            "Predicted Grade: 86.43548463257042 Data: [ 4 29 22 20 23 47 36 50 41] Grade: 87.03\n",
            "Predicted Grade: 73.69331558965337 Data: [ 4 14 13  9 13 46 43 30 29] Grade: 74.67\n",
            "Predicted Grade: 78.97269294149415 Data: [ 3 16 15 25 21 19 50 50 46] Grade: 73.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "zLNf7soq2MWV",
        "outputId": "d4afa5e5-43a1-4549-a7e0-5b235287fb9d"
      },
      "source": [
        "op = x_test+y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-e20d9395e251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (172,9) (172,) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkgiD8B5REYA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "27b633dd-69bf-4d10-be9e-a4750ea6e6e2"
      },
      "source": [
        "ax1 = predictions.plot(kind='scatter', x=predictions, y=y_test, color='red', alpha=0.5, figsize=(10, 7))\n",
        "y_test.plot(kind='scatter', x=predictions, y=y_test, color='blue', alpha=0.5, figsize=(10, 7),ax = ax1)\n",
        "plt.legend(labels=['predict', 'actual'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-b9c32b575861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0max1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scatter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'scatter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'plot'"
          ]
        }
      ]
    }
  ]
}